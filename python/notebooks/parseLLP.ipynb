{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "%run -i myudfs.py\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import datetime\n",
      "\n",
      "from pyspark.sql import SQLContext\n",
      "from pyspark.sql import Row\n",
      "sqC = SQLContext(sc)\n",
      "\n",
      "def fixed_width_row_to_dict(layout,row):\n",
      "    datarow = {}\n",
      "    for var in layout:        \n",
      "        startcol = int(var[0])\n",
      "        startcoln = startcol - 1\n",
      "        varlength = int(var[2])\n",
      "        endcoln = startcoln + varlength\n",
      "        varname = var[1].strip()\n",
      "        datarow[varname] = row[startcoln:endcoln] \n",
      "    return datarow"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
<<<<<<< HEAD
     "prompt_number": 8
=======
     "prompt_number": 19
>>>>>>> 67620ddb525846d00f935cc045929edb15e4269b
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
<<<<<<< HEAD
      "layout2014 = load_csv('/user/daucoin/temp/per/varlayout.tsv',delimiter='\\t', header='true')\n",
=======
      "layout2014 = load_csv('/user/daucoin/temp/per/varlayout2014.tsv',delimiter='\\t', header='true')\n",
>>>>>>> 67620ddb525846d00f935cc045929edb15e4269b
      "bclayout2014 = sc.broadcast(layout2014.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
<<<<<<< HEAD
     "prompt_number": 9
=======
     "prompt_number": 8
>>>>>>> 67620ddb525846d00f935cc045929edb15e4269b
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
<<<<<<< HEAD
      "lines2014 = sc.textFile('/user/daucoin/temp/per/llcp2014.txt')"
=======
      "lines2014 = sc.textFile('/user/daucoin/temp/per/llcp2014.asc')"
>>>>>>> 67620ddb525846d00f935cc045929edb15e4269b
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
<<<<<<< HEAD
     "prompt_number": 11
=======
     "prompt_number": 9
>>>>>>> 67620ddb525846d00f935cc045929edb15e4269b
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "row_rdd = lines2014.map(lambda row: Row(**fixed_width_row_to_dict(bclayout2014.value,row)))\n",
<<<<<<< HEAD
      "df_2014 = sqC.createDataFrame(row_rdd)\n"
=======
      "firstrow = row_rdd.take(1)[0]\n",
      "row_rdd = row_rdd.filter(lambda line: line != firstrow)\n",
      "df_2014 = sqC.createDataFrame(row_rdd)"
>>>>>>> 67620ddb525846d00f935cc045929edb15e4269b
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
<<<<<<< HEAD
     "prompt_number": 25
=======
     "prompt_number": 10
>>>>>>> 67620ddb525846d00f935cc045929edb15e4269b
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#df_2014.printSchema()"
     ],
     "language": "python",
     "metadata": {},
<<<<<<< HEAD
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_2014.select(['IYEAR','_STATE','INCOME2','_BMI5CAT']).head(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "[Row(IYEAR=u'2014', _STATE=u'01', INCOME2=u'07', _BMI5CAT=u'3'),\n",
        " Row(IYEAR=u'2014', _STATE=u'01', INCOME2=u'04', _BMI5CAT=u'2'),\n",
        " Row(IYEAR=u'2014', _STATE=u'01', INCOME2=u'07', _BMI5CAT=u'4'),\n",
        " Row(IYEAR=u'2014', _STATE=u'01', INCOME2=u'05', _BMI5CAT=u'4'),\n",
        " Row(IYEAR=u'2014', _STATE=u'01', INCOME2=u'04', _BMI5CAT=u'4')]"
       ]
      }
     ],
     "prompt_number": 28
=======
     "outputs": [],
     "prompt_number": 22
>>>>>>> 67620ddb525846d00f935cc045929edb15e4269b
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark.sql.functions import lit\n",
      "df_2014 = df_2014.withColumn('year',lit('2014'))\n",
      "df_2014.save(\"data/df_2014_csv\", \"com.databricks.spark.csv\",mode='overwrite', header='true')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
<<<<<<< HEAD
     "prompt_number": 29
=======
     "prompt_number": "*"
>>>>>>> 67620ddb525846d00f935cc045929edb15e4269b
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### START HERE IN THE MORNING #####\n",
      "\n",
      "lines2011 = sc.textFile('/user/daucoin/temp/per/llcp2011.asc')\n",
      "layout2011 = load_csv('/user/daucoin/temp/per/varlayout2011.tsv',delimiter='\\t', header='true')\n",
      "\n",
      "bclayout2011 = sc.broadcast(layout2011.collect())\n",
      "row_rdd = lines2011.map(lambda row: Row(**fixed_width_row_to_dict(bclayout2011.value,row)))\n",
      "firstrow = row_rdd.take(1)[0]\n",
      "row_rdd = row_rdd.filter(lambda line: line != firstrow)\n",
      "df_2011 = sqC.createDataFrame(row_rdd)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
<<<<<<< HEAD
      "df_2011.select('IYEAR').groupby('IYEAR').count().show()\n",
      "df_2011.printSchema()"
=======
      "#df_2011.select('IYEAR').groupby('IYEAR').count().show()\n",
      "#df_2011.printSchema()"
>>>>>>> 67620ddb525846d00f935cc045929edb15e4269b
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
<<<<<<< HEAD
     "prompt_number": 30
=======
     "prompt_number": "*"
>>>>>>> 67620ddb525846d00f935cc045929edb15e4269b
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark.sql.functions import lit\n",
      "df_2011 = df_2011.withColumn('year',lit('2011'))\n",
      "df_2011.save(\"data/df_2011_csv\", \"com.databricks.spark.csv\",mode='overwrite', header='true')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
<<<<<<< HEAD
     "prompt_number": 31
=======
     "prompt_number": "*"
>>>>>>> 67620ddb525846d00f935cc045929edb15e4269b
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_2011 = df_2011.count()\n",
      "n_2014 = df_2014.count()\n",
      "print '2011 count', n_2011\n",
      "print '2014 count', n_2014"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2011 count 506466\n",
        "2014 count 464663\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_2011_2014 = df_2011.unionAll(df_2014)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_2011_2014.count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "971129"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
<<<<<<< HEAD
      "#df_2011_2014.select('IYEAR').groupby('  IYEAR').count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
=======
      "df_2011_2014.select('IYEAR').groupby('  IYEAR').count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "Py4JJavaError",
       "evalue": "An error occurred while calling o141.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve 'year' given input columns CSRVRTRN, HLTHCVR1, LASTSMK2, MAXDRNKS, _RFMAM2Y, GENHLTH, PCPSADI1, EYEEXAM, BLDSUGAR, NUMHHOL2, ASTHMAGE, LENGEXAM, DIABEYE, VETERAN3, _CHISPNC, FEETCHK2, ASBIRDUC, _CPRACE, HEIGHT3, HAVARTH3, ASBIDRNK, ASACTLIM, _RFSIGM2, FEETCHK, PVTRESD1, DRNKANY5, COLGHOUS, HPVADVC2, _RFHLTH, RCSRACE1, EXERANY2, PCPSARS1, SEQNO, SCNTLWK1, LSATISFY, NUMWOMEN, EDUCA, CHILDREN, USEEQUIP, _CRACE1, _STSTR, ASERVIST, DIABAGE2, RCHISLA1, HADSGCO1, ADDEPEV2, _PSU, CHCKIDNY, CSRVTRT1, _IMPHOME, PHYSHLTH, _AGE80, FLUSHOT6, ASBIALCH, _RFPSA21, _DRDXAR1, CARERCVD, _LANDWT, HADSIGM3, CTELENUM, _HISPANC, SCNTMNY1, CSRVCTL1, WTKG3, DROCDY3_, PSATEST1, WTCHSALT, ASATTACK, _RACEGR3, DLYOTHER, PSATIME, _IMPNPH, LSTCOVRG, _STRWT, FALLINJ2, _DRNKMO4, HLTHPLN1, RENTHOM1, SCNTPAID, HPVADSHT, CNCRDIFF, _CASTHM1, CNCRAGE, _CHLDCNT, DIABEDU, _PRACE1, RCSGENDR, SHINGLE2, _HHOLDWT, CASTHDX2, _RFSEAT3, MENTHLTH, BLIND, CVDSTRK3, CSRVINSR, _IMPCRAC, _IMPCAGE, CASTHNO2, HOWLONG, CHCCOPD1, WEIGHT2, HIVTSTD3, QLACTLM2, SEX, NOCOV121, PCPSARE1, TETANUS, _RAWRAKE, _FLSHOT6, _MAM502Y, SLEPTIM1, CNCRTYP1, HADPAP2, _RFDRHV4, _RFDRWM4, CHECKUP1, _IMPRACE, _IMPCSEX, ASTHNOW, MEDBILL1, PREGNANT, _PNEUMO2, _RFBMI5, EMPLOY1, RRCLASS2, PERSDOC2, _LANDWT2, _RFDRMN4, _BMI5CAT, LASTSIG3, NPHH, PREDIAB1, SCNTMEL1, _RFBING5, HTM4, CSRVCLIN, DELAYMED, _RACE, RMVTETH3, USENOW3, CSRVSUM, MEDCOST, PAINACT2, HPVTEST, STATERES, LASTPAP2, LSTBLDS3, DRVISITS, _IMPEDUC, PROFEXAM, IMONTH, BLDSTOOL, HADHYST2, RRATWRK2, IDATE, CVDINFR4, QLMENTL2, ALCDAY5, RREMTSM2, DIFFWALK, FMONTH, HIVTST6, HTIN4, SEATBELT, PDIABTST, DECIDE, HPLSTTST, _RFSEAT2, CSRVDOC1, ASINHALR, FALL12MN, QLSTRES2, NCHH, CSRVINST, NUMADULT, INCOME2, ASBIADVC, RRHCARE3, _ASTHMS1, _AIDTST3, _DENVST2, NUMPHON2, _SMOKER3, ASDRVIST, _DRNKDY4, NAHH, _HCVU651, _RACE_G1, DRADVISE, FLSHTMY2, _AGE_G, _AGEG5YR, LASTDEN3, PCPSAAD2, EMTSUPRT, POORHLTH, DIFFDRES, _AGE65YR, DIFFALON, _MRACE1, LADULT, _RFSMOK3, MEDSCOST, _RFPAP32, _RFBLDS2, SCNTWRK1, DRNKDRI2, SXORIENT, DRNK3GE5, _INCOMG, IYEAR, _LTASTH1, _TOTINDA, NUMMEN, QSTVER, ASYMPTOM, WHRTST10, ASNOSLEP, CHCSCNCR, CVDCRHD4, CSRVPAIN, STOPSMK2, RCSBRAC1, CHCOCNCR, IDAY, SMOKE100, DISPCODE, HADMAM, TRNSGNDR, INTERNET, MEDICARE, LONGWTCH, CPDEMO1, DIABETE3, _EXTETH2, ASRCHKUP, _ALTETH2, RCSRLTN2, SMOKDAY2, _WT2RAKE, SCNTVOT1, _CLANDWT, QSTLANG, _IMPMRTL, PNEUVAC3, _RACEG21, MARITAL, CSRVDEIN, ASBIBING, RRPHYSM2, RCSBIRTH, IMFVPLAC, INSULIN, CHKHEMO3, AVEDRNK2, _BMI5, RRCOGNT2, SCNTLPAD, MSCODE, QLHLTH2, DOCTDIAB, ASTHMA3, _STATE, _EDUCAG, ASTHMED3;\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:63)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:52)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:285)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$transformExpressionUp$1(QueryPlan.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2$$anonfun$apply$2.apply(QueryPlan.scala:123)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:122)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:52)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:50)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:50)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:42)\n\tat org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:920)\n\tat org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:131)\n\tat org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$logicalPlanToDataFrame(DataFrame.scala:154)\n\tat org.apache.spark.sql.DataFrame.select(DataFrame.scala:595)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-21-85de9d843649>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_2011_2014\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/export/apps/spark/latest/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m    736\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Bob'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    737\u001b[0m         \"\"\"\n\u001b[1;32m--> 738\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    739\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    740\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/export/apps/spark/latest/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/export/apps/spark/latest/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
        "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o141.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve 'year' given input columns CSRVRTRN, HLTHCVR1, LASTSMK2, MAXDRNKS, _RFMAM2Y, GENHLTH, PCPSADI1, EYEEXAM, BLDSUGAR, NUMHHOL2, ASTHMAGE, LENGEXAM, DIABEYE, VETERAN3, _CHISPNC, FEETCHK2, ASBIRDUC, _CPRACE, HEIGHT3, HAVARTH3, ASBIDRNK, ASACTLIM, _RFSIGM2, FEETCHK, PVTRESD1, DRNKANY5, COLGHOUS, HPVADVC2, _RFHLTH, RCSRACE1, EXERANY2, PCPSARS1, SEQNO, SCNTLWK1, LSATISFY, NUMWOMEN, EDUCA, CHILDREN, USEEQUIP, _CRACE1, _STSTR, ASERVIST, DIABAGE2, RCHISLA1, HADSGCO1, ADDEPEV2, _PSU, CHCKIDNY, CSRVTRT1, _IMPHOME, PHYSHLTH, _AGE80, FLUSHOT6, ASBIALCH, _RFPSA21, _DRDXAR1, CARERCVD, _LANDWT, HADSIGM3, CTELENUM, _HISPANC, SCNTMNY1, CSRVCTL1, WTKG3, DROCDY3_, PSATEST1, WTCHSALT, ASATTACK, _RACEGR3, DLYOTHER, PSATIME, _IMPNPH, LSTCOVRG, _STRWT, FALLINJ2, _DRNKMO4, HLTHPLN1, RENTHOM1, SCNTPAID, HPVADSHT, CNCRDIFF, _CASTHM1, CNCRAGE, _CHLDCNT, DIABEDU, _PRACE1, RCSGENDR, SHINGLE2, _HHOLDWT, CASTHDX2, _RFSEAT3, MENTHLTH, BLIND, CVDSTRK3, CSRVINSR, _IMPCRAC, _IMPCAGE, CASTHNO2, HOWLONG, CHCCOPD1, WEIGHT2, HIVTSTD3, QLACTLM2, SEX, NOCOV121, PCPSARE1, TETANUS, _RAWRAKE, _FLSHOT6, _MAM502Y, SLEPTIM1, CNCRTYP1, HADPAP2, _RFDRHV4, _RFDRWM4, CHECKUP1, _IMPRACE, _IMPCSEX, ASTHNOW, MEDBILL1, PREGNANT, _PNEUMO2, _RFBMI5, EMPLOY1, RRCLASS2, PERSDOC2, _LANDWT2, _RFDRMN4, _BMI5CAT, LASTSIG3, NPHH, PREDIAB1, SCNTMEL1, _RFBING5, HTM4, CSRVCLIN, DELAYMED, _RACE, RMVTETH3, USENOW3, CSRVSUM, MEDCOST, PAINACT2, HPVTEST, STATERES, LASTPAP2, LSTBLDS3, DRVISITS, _IMPEDUC, PROFEXAM, IMONTH, BLDSTOOL, HADHYST2, RRATWRK2, IDATE, CVDINFR4, QLMENTL2, ALCDAY5, RREMTSM2, DIFFWALK, FMONTH, HIVTST6, HTIN4, SEATBELT, PDIABTST, DECIDE, HPLSTTST, _RFSEAT2, CSRVDOC1, ASINHALR, FALL12MN, QLSTRES2, NCHH, CSRVINST, NUMADULT, INCOME2, ASBIADVC, RRHCARE3, _ASTHMS1, _AIDTST3, _DENVST2, NUMPHON2, _SMOKER3, ASDRVIST, _DRNKDY4, NAHH, _HCVU651, _RACE_G1, DRADVISE, FLSHTMY2, _AGE_G, _AGEG5YR, LASTDEN3, PCPSAAD2, EMTSUPRT, POORHLTH, DIFFDRES, _AGE65YR, DIFFALON, _MRACE1, LADULT, _RFSMOK3, MEDSCOST, _RFPAP32, _RFBLDS2, SCNTWRK1, DRNKDRI2, SXORIENT, DRNK3GE5, _INCOMG, IYEAR, _LTASTH1, _TOTINDA, NUMMEN, QSTVER, ASYMPTOM, WHRTST10, ASNOSLEP, CHCSCNCR, CVDCRHD4, CSRVPAIN, STOPSMK2, RCSBRAC1, CHCOCNCR, IDAY, SMOKE100, DISPCODE, HADMAM, TRNSGNDR, INTERNET, MEDICARE, LONGWTCH, CPDEMO1, DIABETE3, _EXTETH2, ASRCHKUP, _ALTETH2, RCSRLTN2, SMOKDAY2, _WT2RAKE, SCNTVOT1, _CLANDWT, QSTLANG, _IMPMRTL, PNEUVAC3, _RACEG21, MARITAL, CSRVDEIN, ASBIBING, RRPHYSM2, RCSBIRTH, IMFVPLAC, INSULIN, CHKHEMO3, AVEDRNK2, _BMI5, RRCOGNT2, SCNTLPAD, MSCODE, QLHLTH2, DOCTDIAB, ASTHMA3, _STATE, _EDUCAG, ASTHMED3;\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:63)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:52)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:285)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$transformExpressionUp$1(QueryPlan.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2$$anonfun$apply$2.apply(QueryPlan.scala:123)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:122)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:52)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:50)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:50)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:42)\n\tat org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:920)\n\tat org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:131)\n\tat org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$logicalPlanToDataFrame(DataFrame.scala:154)\n\tat org.apache.spark.sql.DataFrame.select(DataFrame.scala:595)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n"
       ]
      }
     ],
     "prompt_number": 21
>>>>>>> 67620ddb525846d00f935cc045929edb15e4269b
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_2011_2014.save(\"data/df_2011_2014_csv\", \"com.databricks.spark.csv\",mode='overwrite', header='true')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "adult_counts = df_2011.select(\"NUMADULT\").groupby(\"NUMADULT\").count()\n",
      "adult_counts.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "+--------+------+\n",
        "|NUMADULT| count|\n",
        "+--------+------+\n",
        "|      5 |    97|\n",
        "|      6 |    27|\n",
        "|      0 | 49389|\n",
        "|      7 |     4|\n",
        "|      1 |341415|\n",
        "|      8 |     2|\n",
        "|      2 | 38564|\n",
        "|      3 |  5066|\n",
        "|        | 71259|\n",
        "|      4 |   643|\n",
        "+--------+------+\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}